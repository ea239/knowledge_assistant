"""
Embedding Model Loader
è´Ÿè´£æä¾›ç»Ÿä¸€çš„ get_embedding_model() æŽ¥å£
å¹¶æ”¯æŒæ¨¡åž‹ç¼“å­˜ã€ä¸åŒ embedding æ¨¡åž‹ç±»åž‹çš„æ‰©å±•ã€‚
"""

from functools import lru_cache
import torch
from sentence_transformers import SentenceTransformer
from loguru import logger # ä½¿ç”¨ loguru æ›¿ä»£ print

# ================================
# ä¸»å…¥å£ï¼šé€šè¿‡æ¨¡åž‹åèŽ·å– embedding æ¨¡åž‹
# ================================
@lru_cache(maxsize=1) # é€šå¸¸åŠ è½½ä¸€ä¸ªå°±å¤Ÿäº†ï¼Œmaxsize=1 èŠ‚çœå†…å­˜
def get_embedding_model(model_name: str = "bge-m3"): # ç»™ä¸ªé»˜è®¤å€¼
    """
    èŽ·å– embedding æ¨¡åž‹ï¼ˆå¸¦ç¼“å­˜ï¼‰ã€‚
    """
    
    # è‡ªåŠ¨æ£€æµ‹åŠ é€Ÿè®¾å¤‡ (GPU > MPS > CPU)
    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    
    logger.info(f"ðŸ–¥ï¸ Inference device: {device}")

    try:
        if model_name == "bge-m3":
            return _load_bge_m3(device)
        
        # å…è®¸ç›´æŽ¥ä¼ å…¥ HuggingFace çš„æ¨¡åž‹ ID (ä½œä¸ºå…œåº•)
        # æ¯”å¦‚ get_embedding_model("shibing624/text2vec-base-chinese")
        logger.info(f"[embedding] Loading generic model: {model_name}...")
        return SentenceTransformer(model_name, device=device)

    except Exception as e:
        logger.error(f"âŒ Failed to load model {model_name}: {e}")
        raise e

# ================================
# æ¨¡åž‹åŠ è½½å‡½æ•°
# ================================

def _load_bge_m3(device: str) -> SentenceTransformer:
    """
    åŠ è½½ BAAI/bge-m3 æ¨¡åž‹ã€‚
    """
    logger.info("[embedding] Loading BGE-M3 model (BAAI/bge-m3)...")
    # [cite_start]è¿™é‡Œå¯¹åº”è®¾è®¡æ–‡æ¡£ä¸­çš„æ¨¡åž‹é€‰åž‹ [cite: 77]
    return SentenceTransformer("BAAI/bge-m3", device=device)