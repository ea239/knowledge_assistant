# api/routes/ingest.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from loguru import logger
from pydantic import BaseModel, HttpUrl  # ğŸ‘ˆ 1. æ–°å¢ HttpUrl ç”¨äºæ ¡éªŒ

from ..db import get_db
from ..models import Article
from ..schemas import IngestTextReq, ArticleOut
from ..services.search_indexer import upsert_article_to_meili

# ğŸ‘ˆ 2. å¼•å…¥ parse_url ä»»åŠ¡
from worker.task import summarize_article, embed_article, parse_url

router = APIRouter()

# --- æ–°å¢çš„è¯·æ±‚æ¨¡å‹ (ä¹Ÿå¯ä»¥æ”¾åˆ° schemas é‡Œï¼Œè¿™é‡Œä¸ºäº†æ–¹ä¾¿ç›´æ¥å†™è¿™äº†) ---
class IngestUrlReq(BaseModel):
    url: HttpUrl

@router.post("/ingest/text", response_model=ArticleOut)
def ingest_text(req: IngestTextReq, db: Session = Depends(get_db)):
    """
    (ä¿æŒä¸å˜) å¯¼å…¥æ–‡æœ¬ -> å…¥åº“ -> è§¦å‘ä»»åŠ¡
    """
    # 1ï¸âƒ£ å»é‡æ£€æŸ¥
    if req.url:
        existing = db.query(Article).filter_by(url=req.url).first()
        if existing:
            raise HTTPException(status_code=400, detail="Article with this URL already exists")

    # 2ï¸âƒ£ æ„å»ºæ–‡ç« å¯¹è±¡
    title = req.title or (req.text[:60] + "..." if len(req.text) > 60 else req.text)
    article = Article(
        url=req.url,
        source_platform=req.source_platform,
        title=title,
        content_text=req.text,
        tags=req.tags or [],
        summary=None,
    )

    # 3ï¸âƒ£ å…¥åº“
    db.add(article)
    db.commit()
    db.refresh(article)

    # 4ï¸âƒ£ æ›´æ–°å…¨æ–‡æ£€ç´¢ç´¢å¼•
    try:
        upsert_article_to_meili(article)
    except Exception as e:
        logger.warning(f"âš ï¸ [Meili] Index update failed: {e}")

    # 5ï¸âƒ£ å¯åŠ¨ Celery å¼‚æ­¥ä»»åŠ¡
    try:
        summarize_article.delay(str(article.id))
        embed_article.delay(str(article.id))
        logger.info(f"ğŸ“¨ [Celery] Queued tasks for {article.id}")
    except Exception as e:
        logger.error(f"âŒ [Celery] Failed to enqueue tasks: {e}")

    return article



@router.post("/ingest/url")
def ingest_url(req: IngestUrlReq):
    """
    æ¥æ”¶ URL -> ä¸¢ç»™ Celery çˆ¬è™« -> ç«‹å³è¿”å›
    åç»­æµç¨‹ï¼šçˆ¬è™«æŠ“å– -> å…¥åº“ -> è‡ªåŠ¨è§¦å‘ Embedding/Summary
    """
    # HttpUrl è½¬å­—ç¬¦ä¸²
    url_str = str(req.url)
    logger.info(f"ğŸ”Œ API received URL to parse: {url_str}")
    
    # ğŸš€ å¼‚æ­¥è§¦å‘è§£æä»»åŠ¡ (Fire and Forget)
    # æˆ‘ä»¬ä¸ç­‰å¾…çˆ¬è™«ç»“æœï¼Œç›´æ¥å‘Šè¯‰ç”¨æˆ·â€œå·²æ¥æ”¶è¯·æ±‚â€
    task = parse_url.delay(url_str)
    
    return {
        "status": "accepted",
        "message": "URL parsing started in background",
        "url": url_str,
        "task_id": str(task.id)
    }