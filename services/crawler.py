import trafilatura
from loguru import logger
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import time
import random

# ğŸŒŸ æ–°å¢ï¼šä½¿ç”¨ curl_cffi æ›¿ä»£æ ‡å‡† requests
from curl_cffi import requests

def normalize_url(url: str) -> str:
    if not url: return ""
    if "#" in url: url = url.split("#")[0]
    return url.strip().rstrip("/")

def detect_platform(url: str) -> str:
    # ... (ä¿æŒä½ ä¹‹å‰çš„ detect_platform ä»£ç ä¸å˜) ...
    domain = urlparse(url).netloc.lower()
    platform_map = {
        "weixin.qq.com": "wechat",
        "zhihu.com": "zhihu",
        "juejin.cn": "juejin",
        "csdn.net": "csdn",
        "baike.baidu.com": "baidu_baike",
        "github.com": "github",
    }
    for key, name in platform_map.items():
        if key in domain: return name
    return "other"

def parse_article_from_url(url: str):
    clean_url = normalize_url(url)
    logger.info(f"ğŸ•·ï¸ Crawling: {clean_url}")

    downloaded = None

    # 1. ä¼˜å…ˆå°è¯• trafilatura (å®ƒå¯¹å¾ˆå¤šæ™®é€šåšå®¢æ”¯æŒå¾ˆå¥½)
    # ä½†å¯¹äºå¾®ä¿¡ï¼Œæˆ‘ä»¬æ•…æ„è®©å®ƒå¤±è´¥æˆ–è·³è¿‡ï¼Œæˆ–è€…ç›´æ¥ç”¨ä¸‹é¢çš„å¢å¼ºè¯·æ±‚
    try:
        # å¦‚æœæ˜¯å¾®ä¿¡ï¼Œtrafilatura å¤§æ¦‚ç‡ä¼šæŒ‚ï¼Œç›´æ¥è·³è¿‡è¿›å…œåº•
        if "weixin" not in clean_url: 
            downloaded = trafilatura.fetch_url(clean_url)
    except Exception:
        pass

    # 2. å¢å¼ºç‰ˆå…œåº• (æ ¸å¿ƒä¿®æ”¹)
    if not downloaded:
        logger.info(f"ğŸš€ Using curl_cffi (Chrome impersonation) for {clean_url}...")
        try:
            # éšæœºä¼‘çœ ï¼Œæ¨¡æ‹ŸçœŸäºº
            time.sleep(random.uniform(1.0, 2.0))
            
            # ğŸŒŸ å…³é”®ï¼šimpersonate="chrome110" ä¼šæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨çš„ TLS æŒ‡çº¹
            # è¿™ä¸€æ­¥èƒ½éª—è¿‡ç»å¤§å¤šæ•°åçˆ¬ (åŒ…æ‹¬éƒ¨åˆ†å¾®ä¿¡é£æ§)
            resp = requests.get(
                clean_url, 
                impersonate="chrome110", 
                timeout=15,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
                }
            )
            
            if resp.status_code == 200:
                # å¾®ä¿¡ç‰¹å®šçš„ç¼–ç å¤„ç†
                if "weixin" in clean_url:
                     resp.encoding = "utf-8"
                downloaded = resp.text
            else:
                logger.error(f"âŒ Request failed: {resp.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Download failed: {e}")
            return None

    # 3. æå–å†…å®¹
    if not downloaded: return None
    
    # ğŸŒŸ æ–°å¢ï¼šåƒåœ¾æ•°æ®æ£€æµ‹
    # å¦‚æœæŠ“ä¸‹æ¥æ˜¯éªŒè¯ç é¡µé¢ï¼Œç›´æ¥æ”¾å¼ƒï¼Œä¸è¦å…¥åº“
    if "wappoc_appmsgcaptcha" in downloaded or "<title>ç¯å¢ƒå¼‚å¸¸</title>" in downloaded:
        logger.warning("â›” Detected WeChat CAPTCHA/Block. Skipping to avoid garbage data.")
        return None

    try:
        result_json = trafilatura.extract(
            downloaded, 
            include_comments=False, 
            include_tables=True, 
            output_format="json", 
            with_metadata=True
        )

        parsed_data = {}
        if result_json:
            import json
            data = json.loads(result_json)
            parsed_data = {
                "title": data.get("title") or "Untitled",
                "content_text": data.get("text"),
                "source_platform": detect_platform(clean_url),
                "url": clean_url
            }
        else:
            # BS4 å…œåº•
            soup = BeautifulSoup(downloaded, "html.parser")
            title = soup.title.string.strip() if soup.title else "Untitled"
            parsed_data = {
                "title": title,
                "content_text": "", 
                "source_platform": detect_platform(clean_url),
                "url": clean_url
            }
        
        # å†æ¬¡æ£€æŸ¥æ ‡é¢˜æ˜¯å¦æ­£å¸¸
        if parsed_data["title"] in ["ç¯å¢ƒå¼‚å¸¸", "è®¿é—®è¿‡äºé¢‘ç¹"]:
             logger.warning("â›” Detected blocked title. Skipping.")
             return None

        logger.success(f"âœ… Parsed: {parsed_data['title']}")
        return parsed_data

    except Exception as e:
        logger.error(f"âŒ Parse error: {e}")
        return None